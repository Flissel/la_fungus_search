# file: ..\sakana-desktop-assistant\src\core\llm_interface.py | lines: 1-243 | window: 4000
import asyncio
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, AsyncGenerator
from pathlib import Path
import logging

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

logger = logging.getLogger(__name__)

class LLMInterface(ABC):
    """Abstract base class for LLM interfaces"""
    
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate a response from the LLM"""
        pass
    
    @abstractmethod
    async def stream_generate(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        """Stream a response from the LLM"""
        pass
    
    @abstractmethod
    async def embed(self, text: str) -> List[float]:
        """Generate embeddings for text"""
        pass

class LocalLLM(LLMInterface):
    """Local LLM using llama.cpp"""
    
    def __init__(self, model_path: Path, **kwargs):
        try:
            from llama_cpp import Llama
            # Create main model for generation
            self.llm = Llama(
                model_path=str(model_path),
                n_ctx=kwargs.get('context_size', 4096),
                n_threads=kwargs.get('threads', 8),
                n_gpu_layers=kwargs.get('gpu_layers', 0),
                embedding=False,  # Disable embeddings to avoid issues
                verbose=False  # Suppress llama_print_timings output
            )
            # Try to create separate embedding model, fall back to simple embeddings if fails
            try:
                self.embed_model = Llama(
                    model_path=str(model_path),
                    n_ctx=512,  # Smaller context for embeddings
                    embedding=True,
                    n_threads=kwargs.get('threads', 8),
                    verbose=False  # Suppress llama_print_timings output
                )
                self.use_model_embeddings = True
            except:
                logger.warning("Could not create embedding model, using fallback embeddings")
                self.use_model_embeddings = False
                
        except ImportError:
            raise ImportError("llama-cpp-python not installed")
        
        self.temperature = kwargs.get('temperature', 0.7)
        self.max_tokens = kwargs.get('max_tokens', 2048)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate response using local LLM"""
        loop = asyncio.get_event_loop()
        
        def _generate():
            try:
                logger.info(f"Generating with prompt length: {len(prompt)}")
                # Add proper stop tokens for Llama
                stop_tokens = kwargs.get('stop', [])
                if hasattr(self.llm, 'metadata') and 'tokenizer.ggml.eos_token_id' in self.llm.metadata:
                    stop_tokens.extend(["<|eot_id|>", "<|end_of_text|>"])
                
                response = self.llm(
                    prompt,
                    max_tokens=kwargs.get('max_tokens', self.max_tokens),
                    temperature=kwargs.get('temperature', self.temperature),
                    stop=stop_tokens,
                    echo=False  # Don't repeat the prompt
                )
                
                if not response or 'choices' not in response:
                    logger.error(f"Invalid response structure: {response}")
                    return "I apologize, but I'm having trouble generating a response. Please try again."
                
                text = response['choices'][0].get('text', '')
                logger.info(f"Generated response length: {len(text)}")
                
                if not text or text.isspace():
                    return "Hello! I'm Sakana, your self-learning desktop assistant. How can I help you today?"
                
                return text.strip()
                
            except Exception as e:
                logger.error(f"Generation error: {e}")
                return f"I encountered an error: {str(e)}. Please try again."
        
        return await loop.run_in_executor(None, _generate)
    
    async def stream_generate(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        """Stream response from local LLM"""
        loop = asyncio.get_event_loop()
        
        def _stream():
            stream = self.llm(
                prompt,
                max_tokens=kwargs.get('max_tokens', self.max_tokens),
                temperature=kwargs.get('temperature', self.temperature),
                stop=kwargs.get('stop', []),
                stream=True
            )
            for output in stream:
                yield output['choices'][0]['text']
        
        for chunk in await loop.run_in_executor(None, lambda: list(_stream())):
            yield chunk
    
    async def embed(self, text: str) -> List[float]:
        """Generate embeddings using local model"""
        if self.use_model_embeddings:
            loop = asyncio.get_event_loop()
            
            def _embed():
                return self.embed_model.embed(text)
            
            try:
                return await loop.run_in_executor(None, _embed)
            except Exception as e:
                logger.warning(f"Model embedding failed, using fallback: {e}")
                self.use_model_embeddings = False
        
        # Fallback: Simple hash-based embeddings
        import hashlib
        hash_obj = hashlib.sha256(text.encode())
        hash_bytes = hash_obj.digest()
        
        # Convert to normalized floats
        embedding = []
        for i in range(0, min(len(hash_bytes), 96), 3):
            if i + 2 < len(hash_bytes):
                value = (hash_bytes[i] + hash_bytes[i+1] + hash_bytes[i+2]) / (255.0 * 3)
                embedding.append(value * 2 - 1)  # Normalize to [-1, 1]
        
        # Pad to 384 dimensions
        while len(embedding) < 384:
            embedding.append(0.0)
            
        return embedding[:384]

class OpenAILLM(LLMInterface):
    """OpenAI API interface"""
    
    def __init__(self, api_key: str, model: str = "gpt-4", **kwargs):
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAI package not installed. Install with: pip install openai")
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.model = model
        self.temperature = kwargs.get('temperature', 0.7)
        self.max_tokens = kwargs.get('max_tokens', 2048)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate response using OpenAI"""
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=kwargs.get('temperature', self.temperature),
                max_tokens=kwargs.get('max_tokens', self.max_tokens)
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"OpenAI generation failed: {e}")
            raise
    
    async def stream_generate(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        """Stream response from OpenAI"""
        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=kwargs.get('temperature', self.temperature),
                max_tokens=kwargs.get('max_tokens', self.max_tokens),
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            logger.error(f"OpenAI streaming failed: {e}")
            raise
    
    async def embed(self, text: str) -> List[float]:
        """Generate embeddings using OpenAI"""
        try:
            response = await self.client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"OpenAI embedding failed: {e}")
            raise

class LLMFactory:
    """Factory for creating LLM interfaces"""
    
    @staticmethod
    def create(config: 'Config') -> LLMInterface:
        """Create LLM interface based on configuration"""
        if config.llm_provider == "local":
            model_path = config.models_dir / f"{config.model_name}.gguf"
            if not model_path.exists():
                logger.warning(f"Model not found: {model_path}, using mock LLM for testing")
                from .llm_interface_minimal import MockLLM
                return MockLLM()
            return LocalLLM(
                model_path=model_path,
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )
        elif config.llm_provider == "openai":
            if not config.api_key:
                logger.warning("OpenAI API key not provided, using mock LLM for testing")
                from .llm_interface_minimal import MockLLM
                return MockLLM()
            return OpenAILLM(
                api_key=config.api_key,
                model=config.model_name,
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )
        elif config.llm_provider == "mock":
            from .llm_interface_minimal import MockLLM
            return MockLLM()
        else:
            raise ValueError(f"Unknown LLM provider: {config.llm_provider}")
