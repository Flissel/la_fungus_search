# file: ..\sakana-desktop-assistant\src\core\llm_interface_minimal.py | lines: 1-81 | window: 100
"""Minimal LLM interface for testing without heavy dependencies"""

import asyncio
from typing import List, Dict, Any, AsyncGenerator
import random
import logging
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)

class LLMInterface(ABC):
    """Abstract base class for LLM interfaces"""
    
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate a response from the LLM"""
        pass
    
    @abstractmethod
    async def stream_generate(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        """Stream a response from the LLM"""
        pass
    
    @abstractmethod
    async def embed(self, text: str) -> List[float]:
        """Generate embeddings for text"""
        pass

class MockLLM(LLMInterface):
    """Mock LLM for testing without actual models"""
    
    def __init__(self, **kwargs):
        self.responses = [
            "I understand your request. Let me help you with that.",
            "I'm learning from our interactions to better assist you.",
            "Based on your patterns, I suggest trying this approach.",
            "I've noted your preference and will remember it for next time.",
        ]
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate a mock response"""
        await asyncio.sleep(0.5)  # Simulate processing time
        
        # Simple response based on prompt content
        if "help" in prompt.lower():
            return "I'm here to help! I'm a self-learning assistant that adapts to your needs. Try asking me to manage files, check system info, or create tasks."
        elif "file" in prompt.lower() or "ls" in prompt.lower():
            return "To list files, use the 'ls' command. To read a file, use 'cat filename'. I'm learning your file management patterns."
        elif "system" in prompt.lower() or "cpu" in prompt.lower():
            return "Use 'sysinfo' for system overview, 'cpu' for CPU usage, or 'memory' for memory stats."
        else:
            return random.choice(self.responses)
    
    async def stream_generate(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        """Stream a mock response"""
        response = await self.generate(prompt, **kwargs)
        words = response.split()
        
        for word in words:
            yield word + " "
            await asyncio.sleep(0.05)
    
    async def embed(self, text: str) -> List[float]:
        """Generate mock embeddings"""
        # Simple hash-based embeddings for testing
        import hashlib
        hash_obj = hashlib.md5(text.encode())
        hash_hex = hash_obj.hexdigest()
        
        # Convert to float vector
        embedding = []
        for i in range(0, len(hash_hex), 4):
            chunk = hash_hex[i:i+4]
            value = int(chunk, 16) / 65535.0  # Normalize to 0-1
            embedding.append(value)
        
        # Pad to 384 dimensions (typical small embedding size)
        while len(embedding) < 384:
            embedding.append(0.0)
        
        return embedding[:384]
