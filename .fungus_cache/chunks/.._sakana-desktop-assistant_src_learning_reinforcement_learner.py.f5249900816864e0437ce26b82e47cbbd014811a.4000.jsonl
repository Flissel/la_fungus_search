# file: ..\sakana-desktop-assistant\src\learning\reinforcement_learner.py | lines: 1-297 | window: 4000
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
from collections import defaultdict, deque
import random
import json
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class ReinforcementLearner:
    """Reinforcement learning for improving assistant behavior"""
    
    def __init__(
        self,
        learning_rate: float = 0.01,
        discount_factor: float = 0.95,
        exploration_rate: float = 0.1,
        memory_size: int = 1000
    ):
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.memory_size = memory_size
        
        # Q-table for state-action values
        self.q_table = defaultdict(lambda: defaultdict(float))
        
        # Experience replay memory
        self.memory = deque(maxlen=memory_size)
        
        # Episode tracking
        self.current_episode = []
        self.episode_count = 0
        
        # Reward signals
        self.reward_signals = {
            'positive_feedback': 1.0,
            'negative_feedback': -1.0,
            'task_completed': 0.5,
            'error_occurred': -0.5,
            'fast_response': 0.2,
            'slow_response': -0.2,
            'pattern_learned': 0.3
        }
    
    def get_state_representation(self, context: Dict[str, Any]) -> str:
        """Convert context to state representation"""
        
        # Extract key features for state
        state_features = {
            'input_type': context.get('input_type', 'unknown'),
            'time_of_day': self._get_time_category(context.get('timestamp', '')),
            'conversation_length': min(len(context.get('conversation_history', [])), 5),
            'has_code': 'code' in context.get('user_input', '').lower(),
            'previous_success': context.get('previous_success', True)
        }
        
        # Create state string
        return json.dumps(state_features, sort_keys=True)
    
    def get_action_space(self) -> List[str]:
        """Get available actions"""
        
        return [
            'direct_response',
            'search_memory',
            'execute_code',
            'ask_clarification',
            'provide_examples',
            'suggest_alternatives',
            'detailed_explanation',
            'concise_response'
        ]
    
    def choose_action(self, state: str, available_actions: Optional[List[str]] = None) -> str:
        """Choose action using epsilon-greedy strategy"""
        
        if available_actions is None:
            available_actions = self.get_action_space()
        
        # Exploration
        if random.random() < self.exploration_rate:
            return random.choice(available_actions)
        
        # Exploitation - choose best action
        state_values = self.q_table[state]
        
        # Filter to available actions
        action_values = {
            action: state_values.get(action, 0.0)
            for action in available_actions
        }
        
        if not action_values:
            return random.choice(available_actions)
        
        # Return action with highest Q-value
        return max(action_values.items(), key=lambda x: x[1])[0]
    
    def record_experience(
        self,
        state: str,
        action: str,
        reward: float,
        next_state: str,
        done: bool = False
    ):
        """Record experience for replay"""
        
        experience = {
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state,
            'done': done
        }
        
        self.memory.append(experience)
        self.current_episode.append(experience)
        
        if done:
            self._process_episode()
    
    def calculate_reward(self, feedback: Dict[str, Any]) -> float:
        """Calculate reward from feedback signals"""
        
        reward = 0.0
        
        # User feedback
        if feedback.get('user_satisfied'):
            reward += self.reward_signals['positive_feedback']
        elif feedback.get('user_frustrated'):
            reward += self.reward_signals['negative_feedback']
        
        # Task completion
        if feedback.get('task_completed'):
            reward += self.reward_signals['task_completed']
        
        # Errors
        if feedback.get('error_occurred'):
            reward += self.reward_signals['error_occurred']
        
        # Response time
        response_time = feedback.get('response_time', 1.0)
        if response_time < 0.5:
            reward += self.reward_signals['fast_response']
        elif response_time > 3.0:
            reward += self.reward_signals['slow_response']
        
        # Learning progress
        if feedback.get('pattern_detected'):
            reward += self.reward_signals['pattern_learned']
        
        return reward
    
    def update_q_value(self, state: str, action: str, reward: float, next_state: str):
        """Update Q-value using Q-learning algorithm"""
        
        current_q = self.q_table[state][action]
        
        # Get max Q-value for next state
        next_actions = self.q_table[next_state]
        max_next_q = max(next_actions.values()) if next_actions else 0.0
        
        # Q-learning update
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        self.q_table[state][action] = new_q
    
    def learn_from_replay(self, batch_size: int = 32):
        """Learn from replay memory"""
        
        if len(self.memory) < batch_size:
            return
        
        # Sample batch from memory
        batch = random.sample(self.memory, batch_size)
        
        for experience in batch:
            self.update_q_value(
                experience['state'],
                experience['action'],
                experience['reward'],
                experience['next_state']
            )
    
    def _process_episode(self):
        """Process completed episode"""
        
        if not self.current_episode:
            return
        
        # Calculate cumulative rewards
        cumulative_reward = 0
        for i in reversed(range(len(self.current_episode))):
            experience = self.current_episode[i]
            cumulative_reward = experience['reward'] + self.discount_factor * cumulative_reward
            
            # Update with cumulative reward
            self.update_q_value(
                experience['state'],
                experience['action'],
                cumulative_reward,
                experience['next_state']
            )
        
        self.episode_count += 1
        self.current_episode = []
        
        # Decay exploration rate
        self.exploration_rate = max(0.01, self.exploration_rate * 0.995)
        
        logger.info(f"Episode {self.episode_count} completed. Exploration rate: {self.exploration_rate:.3f}")
    
    def _get_time_category(self, timestamp: str) -> str:
        """Categorize time of day"""
        
        try:
            from datetime import datetime
            dt = datetime.fromisoformat(timestamp)
            hour = dt.hour
            
            if 6 <= hour < 12:
                return 'morning'
            elif 12 <= hour < 17:
                return 'afternoon'
            elif 17 <= hour < 22:
                return 'evening'
            else:
                return 'night'
        except:
            return 'unknown'
    
    def get_policy_stats(self) -> Dict[str, Any]:
        """Get statistics about learned policy"""
        
        stats = {
            'states_explored': len(self.q_table),
            'total_experiences': len(self.memory),
            'episodes_completed': self.episode_count,
            'exploration_rate': self.exploration_rate,
            'top_actions_by_state': {}
        }
        
        # Get top action for each state
        for state, actions in list(self.q_table.items())[:5]:  # Top 5 states
            if actions:
                best_action = max(actions.items(), key=lambda x: x[1])
                stats['top_actions_by_state'][state] = {
                    'action': best_action[0],
                    'q_value': best_action[1]
                }
        
        return stats
    
    def save_model(self, path: Path):
        """Save Q-table and parameters"""
        
        model_data = {
            'q_table': dict(self.q_table),
            'learning_rate': self.learning_rate,
            'discount_factor': self.discount_factor,
            'exploration_rate': self.exploration_rate,
            'episode_count': self.episode_count
        }
        
        with open(path, 'w') as f:
            json.dump(model_data, f, indent=2)
    
    def load_model(self, path: Path):
        """Load Q-table and parameters"""
        
        if not path.exists():
            logger.warning(f"Model file not found: {path}")
            return
        
        try:
            with open(path, 'r') as f:
                model_data = json.load(f)
            
            self.q_table = defaultdict(lambda: defaultdict(float))
            for state, actions in model_data['q_table'].items():
                for action, value in actions.items():
                    self.q_table[state][action] = value
            
            self.learning_rate = model_data.get('learning_rate', self.learning_rate)
            self.discount_factor = model_data.get('discount_factor', self.discount_factor)
            self.exploration_rate = model_data.get('exploration_rate', self.exploration_rate)
            self.episode_count = model_data.get('episode_count', 0)
            
            logger.info(f"Loaded RL model with {len(self.q_table)} states")
            
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
