# file: ..\sakana-desktop-assistant\src\core\assistant.py | lines: 1-100 | window: 100
import asyncio
import json
import re
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime
from pathlib import Path
import logging

from .config import Config
from .llm_interface import LLMFactory
from ..memory import MemoryManager, Memory, MemoryType, PatternDetector
from ..learning import EvolutionaryLearner, SelfModifier, ReinforcementLearner
from ..execution import SandboxExecutor
from ..plugins import PluginManager

logger = logging.getLogger(__name__)

class SakanaAssistant:
    """Main assistant class with self-learning capabilities"""
    
    def __init__(self, config: Config):
        self.config = config
        
        # Core components
        self.llm = None
        self.memory_manager = None
        self.pattern_detector = None
        self.evolutionary_learner = None
        self.self_modifier = None
        self.sandbox_executor = None
        self.plugin_manager = None
        
        # State
        self.is_initialized = False
        self.conversation_history = []
        self.current_context = {}
        self.learning_enabled = True
        
        # Performance tracking
        self.metrics = {
            'requests_handled': 0,
            'successful_completions': 0,
            'errors': 0,
            'average_response_time': 0.0,
            'patterns_learned': 0,
            'self_improvements': 0
        }
        
        # Behavior genome for evolved capabilities
        self.behavior_genome = {}
        self.evolution_trigger = None
        
        # File discovery learner
        self.file_discovery_learner = None
        
        # Project discovery learner
        self.project_discovery_learner = None
    
    async def initialize(self):
        """Initialize all components"""
        logger.info("Initializing Sakana Desktop Assistant...")
        
        # Initialize LLM
        self.llm = LLMFactory.create(self.config)
        
        # Initialize memory system
        self.memory_manager = MemoryManager(
            db_path=self.config.memory_db_path,
            llm_interface=self.llm
        )
        await self.memory_manager.initialize()
        
        # Initialize pattern detector
        self.pattern_detector = PatternDetector(self.memory_manager)
        
        # Initialize learning systems
        self.evolutionary_learner = EvolutionaryLearner(
            population_size=self.config.population_size,
            mutation_rate=self.config.mutation_rate,
            archive_path=self.config.data_dir / "evolution_archive.json"
        )
        self.evolutionary_learner.load_archive()
        
        self.self_modifier = SelfModifier(
            sandbox_enabled=self.config.sandbox_enabled,
            modifications_dir=self.config.data_dir / "modifications"
        )
        
        # Initialize evolution triggers
        from ..learning import EvolutionTrigger, FileDiscoveryLearner
        self.evolution_trigger = EvolutionTrigger(self)
        
        # Initialize file discovery learner with cache
        cache_dir = self.config.data_dir / "cache"
        self.file_discovery_learner = FileDiscoveryLearner(cache_dir=str(cache_dir))
        # Start discovery process in background
        asyncio.create_task(self._initialize_file_discovery())
        
        # Initialize project discovery learner
        from ..learning.project_discovery import ProjectDiscoveryLearner

# file: ..\sakana-desktop-assistant\src\core\assistant.py | lines: 101-200 | window: 100
        self.project_discovery_learner = ProjectDiscoveryLearner()
        # Start project discovery in background (after file discovery)
        asyncio.create_task(self._initialize_project_discovery())
        
        # Initialize execution sandbox
        if self.config.sandbox_enabled:
            self.sandbox_executor = SandboxExecutor(
                max_execution_time=self.config.max_execution_time
            )
            await self.sandbox_executor.initialize()
        
        # Initialize plugin system
        self.plugin_manager = PluginManager(self.config.plugins_dir)
        await self.plugin_manager.load_plugins()
        
        # Load user preferences and patterns
        await self._load_user_profile()
        
        self.is_initialized = True
        logger.info("Assistant initialized successfully")
    
    async def _initialize_file_discovery(self):
        """Initialize file discovery in background"""
        try:
            # Discover available commands
            await self.file_discovery_learner.discover_file_commands()
            # Learn about environment
            await self.file_discovery_learner.learn_from_environment()
            
            learned = self.file_discovery_learner.get_learned_summary()
            logger.info(f"File discovery initialized: {learned}")
        except Exception as e:
            logger.error(f"Error in file discovery initialization: {e}")
    
    async def _initialize_project_discovery(self):
        """Initialize project discovery in background"""
        try:
            # Wait a bit for file discovery to complete
            await asyncio.sleep(10)
            
            # Discover projects in home directory
            logger.info("Starting project discovery...")
            projects = await self.project_discovery_learner.discover_projects()
            
            logger.info(f"Project discovery complete: found {len(projects)} projects")
            
            # Store in memory for quick access
            for project in projects:
                memory = Memory(
                    type=MemoryType.LONG_TERM,
                    content=f"Project: {project.name} at {project.path}",
                    context={
                        'type': 'discovered_project',
                        'project_name': project.name,
                        'project_type': project.type,
                        'capabilities': project.capabilities
                    }
                )
                await self.memory_manager.store_memory(memory)
                
        except Exception as e:
            logger.error(f"Error in project discovery initialization: {e}")
    
    async def process_request(self, user_input: str) -> Dict[str, Any]:
        """Process a user request with learning and adaptation"""
        
        start_time = datetime.now()
        self.metrics['requests_handled'] += 1
        
        try:
            # Store in short-term memory
            input_memory = Memory(
                type=MemoryType.SHORT_TERM,
                content=user_input,
                context={'type': 'user_input', 'timestamp': start_time.isoformat()}
            )
            await self.memory_manager.store_memory(input_memory)
            
            # Detect patterns in user behavior
            await self._detect_user_patterns(user_input)
            
            # Retrieve relevant memories
            relevant_memories = await self.memory_manager.retrieve_memories(
                query=user_input,
                limit=10
            )
            
            # Build context
            context = self._build_context(user_input, relevant_memories)
            
            # Check for plugin commands
            plugin_response = await self.plugin_manager.handle_command(user_input, context)
            if plugin_response:
                response = plugin_response
            else:
                # Generate response using LLM
                response = await self._generate_response(user_input, context)
            
            # Execute any requested actions
            if self._contains_code_request(response):

# file: ..\sakana-desktop-assistant\src\core\assistant.py | lines: 201-300 | window: 100
                execution_result = await self._execute_code(response)
                response = self._merge_execution_result(response, execution_result)
            
            # Store response in memory
            response_memory = Memory(
                type=MemoryType.SHORT_TERM,
                content=response['content'],
                context={
                    'type': 'assistant_response',
                    'user_input': user_input,
                    'timestamp': datetime.now().isoformat()
                }
            )
            await self.memory_manager.store_memory(response_memory)
            
            # Learn from interaction
            if self.learning_enabled:
                await self._learn_from_interaction(user_input, response)
            
            # Update metrics
            self.metrics['successful_completions'] += 1
            response_time = (datetime.now() - start_time).total_seconds()
            self._update_avg_response_time(response_time)
            
            return {
                'success': True,
                'response': response,
                'response_time': response_time,
                'memories_used': len(relevant_memories)
            }
            
        except Exception as e:
            logger.error(f"Error processing request: {e}")
            self.metrics['errors'] += 1
            
            # Detect task type from user input
            task_type = self._detect_task_type(user_input)
            
            # Trigger evolution on failure
            if self.evolution_trigger and task_type:
                await self.evolution_trigger.on_task_failure(
                    task_type=task_type,
                    context={'user_input': user_input, 'error': str(e)},
                    error=str(e)
                )
            
            return {
                'success': False,
                'error': str(e),
                'response': {
                    'content': "I encountered an error processing your request. Let me try a different approach.",
                    'type': 'error'
                }
            }
    
    async def _detect_user_patterns(self, user_input: str):
        """Detect patterns in user behavior"""
        
        # Time-based patterns
        current_hour = datetime.now().hour
        await self.memory_manager.detect_pattern(
            'time_preference',
            {'hour': current_hour, 'input_type': self._classify_input(user_input)}
        )
        
        # Command patterns
        if any(keyword in user_input.lower() for keyword in ['create', 'make', 'build']):
            await self.memory_manager.detect_pattern(
                'command_type',
                {'type': 'creation', 'keywords': self._extract_keywords(user_input)}
            )
        
        # Topic patterns
        topics = self._extract_topics(user_input)
        for topic in topics:
            await self.memory_manager.detect_pattern(
                'topic_interest',
                {'topic': topic}
            )
    
    async def _generate_response(self, user_input: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate response using LLM with context"""
        
        # Check if we have evolved behaviors for this task type
        task_type = self._detect_task_type(user_input)
        
        if task_type in self.behavior_genome:
            # Try evolved behavior first
            evolved_response = await self._try_evolved_behavior(task_type, user_input, context)
            if evolved_response and evolved_response.get('success'):
                return evolved_response
        
        # Build prompt with context
        prompt = self._build_prompt(user_input, context)
        
        # Generate response
        response_text = await self.llm.generate(prompt)
        
        # Parse response for structured data
        response_data = self._parse_response(response_text)

# file: ..\sakana-desktop-assistant\src\core\assistant.py | lines: 301-400 | window: 100
        
        return {
            'content': response_data.get('content', response_text),
            'type': response_data.get('type', 'text'),
            'actions': response_data.get('actions', []),
            'metadata': response_data.get('metadata', {})
        }
    
    async def _try_evolved_behavior(self, task_type: str, user_input: str, context: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Try to use evolved behavior for task"""
        
        genome = self.behavior_genome.get(task_type)
        if not genome:
            return None
        
        try:
            # Extract file path if it's a document task
            if task_type == 'document_summarization':
                import re
                # Try to extract file path from user input
                path_patterns = [
                    r'"([^"]+)"',  # Quoted path
                    r'([A-Za-z]:\\[^\\]+(?:\\[^\\]+)*)',  # Windows path
                    r'(/[^/]+(?:/[^/]+)*)',  # Unix path
                ]
                
                file_path = None
                for pattern in path_patterns:
                    match = re.search(pattern, user_input)
                    if match:
                        file_path = match.group(1)
                        break
                
                if file_path:
                    # Use file discovery learner to evolve search strategy
                    if self.file_discovery_learner:
                        logger.info(f"Using file discovery learner to find: {file_path}")
                        
                        # Check if we're in crisis mode (user detected hallucination)
                        verification_required = getattr(self, 'verification_required', False)
                        
                        search_result = await self.file_discovery_learner.evolve_search_strategy(
                            file_path, 
                            verification_required=verification_required
                        )
                        
                        if search_result and search_result.get('found'):
                            found_path = search_result['path']
                            logger.info(f"File found through evolution at: {found_path}")
                            
                            # If we already have content from verification, use it
                            if search_result.get('content'):
                                content = search_result['content']
                                summary = await self._generate_summary(content)
                                
                                # Include proof of access
                                proof = ""
                                if search_result.get('verified'):
                                    proof = f"\n\n**Proof of Access:**\nFirst line: {search_result.get('first_line', content[:100])}..."
                                
                                return {
                                    'success': True,
                                    'content': f"Successfully accessed file through self-evolution!\n\n{summary}{proof}",
                                    'type': 'text',
                                    'metadata': {
                                        'evolved_behavior': True,
                                        'path_discovered': found_path,
                                        'strategy_used': search_result.get('strategy'),
                                        'verified': search_result.get('verified', False)
                                    }
                                }
                            else:
                                # Read the discovered file
                                result = await self.plugin_manager.handle_command(f'cat {found_path}', context)
                                if result and result.get('type') != 'error':
                                    content = result.get('content', '')
                                    summary = await self._generate_summary(content)
                                    return {
                                        'success': True,
                                        'content': f"Found file through self-evolution!\n\n{summary}",
                                        'type': 'text',
                                        'metadata': {
                                            'evolved_behavior': True,
                                            'path_discovered': found_path,
                                            'strategy_used': search_result.get('strategy')
                                        }
                                    }
                    
                    # Fallback to genome-based approach
                    if genome.get('path_handling') == 'smart':
                        # Try multiple path variations
                        wsl_path = file_path[3:].replace('\\', '/') if file_path.startswith('C:\\') else file_path
                        path_variations = [
                            file_path,
                            file_path.replace('\\', '/'),
                            f"/mnt/c/{wsl_path}" if file_path.startswith('C:\\') else file_path,
                        ]
                        
                        for path in path_variations:
                            try:

# file: ..\sakana-desktop-assistant\src\core\assistant.py | lines: 401-500 | window: 100
                                # Use the file reading method from genome
                                if genome.get('file_reading_method') == 'cat':
                                    result = await self.plugin_manager.handle_command(f'cat {path}', context)
                                    if result and result.get('type') != 'error':
                                        # Successfully read file, now summarize
                                        content = result.get('content', '')
                                        summary = await self._generate_summary(content)
                                        return {
                                            'success': True,
                                            'content': summary,
                                            'type': 'text',
                                            'metadata': {'evolved_behavior': True, 'path_used': path}
                                        }
                            except Exception as e:
                                logger.debug(f"Path variation {path} failed: {e}")
                                continue
            
        except Exception as e:
            logger.error(f"Evolved behavior failed: {e}")
        
        return None
    
    async def _generate_summary(self, content: str) -> str:
        """Generate a summary of content"""
        
        if not content:
            return "Unable to read file content."
        
        # Limit content length for summarization
        max_chars = 2000
        if len(content) > max_chars:
            content = content[:max_chars] + "... (truncated)"
        
        summary_prompt = f"Summarize the following content concisely:\n\n{content}\n\nSummary:"
        summary = await self.llm.generate(summary_prompt, max_tokens=150)
        
        return summary
    
    async def _execute_code(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Execute code in sandbox if requested"""
        
        if not self.sandbox_executor:
            return {'success': False, 'error': 'Sandbox not enabled'}
        
        code = self._extract_code_from_response(response)
        if not code:
            return {'success': False, 'error': 'No code found'}
        
        # Execute in sandbox
        result = await self.sandbox_executor.execute(code)
        
        return result
    
    async def _learn_from_interaction(self, user_input: str, response: Dict[str, Any]):
        """Learn from the interaction to improve future responses"""
        
        # Update conversation patterns
        self.conversation_history.append({
            'user': user_input,
            'assistant': response,
            'timestamp': datetime.now().isoformat()
        })
        
        # Check for immediate feedback
        if self.evolution_trigger:
            feedback_type = self.evolution_trigger.analyze_user_feedback(user_input, self.current_context)
            
            if feedback_type == 'crisis':
                # HALLUCINATION CRISIS - Maximum priority evolution
                task_type = self._detect_task_type(self.conversation_history[-2]['user'] if len(self.conversation_history) > 1 else user_input)
                
                if task_type:
                    logger.critical(f"CRISIS MODE: Hallucination detected for {task_type}")
                    await self.evolution_trigger.on_task_failure(
                        task_type=task_type,
                        context={
                            'user_input': self.conversation_history[-2]['user'] if len(self.conversation_history) > 1 else user_input,
                            'assistant_response': self.conversation_history[-2]['assistant'] if len(self.conversation_history) > 1 else response,
                            'user_feedback': user_input,
                            'crisis_mode': True
                        },
                        error="HALLUCINATION: User detected false claims"
                    )
            elif feedback_type == 'negative':
                # User indicated something was wrong - trigger immediate evolution
                task_type = self._detect_task_type(self.conversation_history[-2]['user'] if len(self.conversation_history) > 1 else user_input)
                
                if task_type:
                    logger.info(f"Negative feedback detected, triggering evolution for {task_type}")
                    await self.evolution_trigger.on_task_failure(
                        task_type=task_type,
                        context={
                            'user_input': self.conversation_history[-2]['user'] if len(self.conversation_history) > 1 else user_input,
                            'assistant_response': self.conversation_history[-2]['assistant'] if len(self.conversation_history) > 1 else response,
                            'user_feedback': user_input
                        },
                        error="User indicated incorrect response"
                    )
            elif feedback_type == 'positive':
                # Positive reinforcement - strengthen current approach

# file: ..\sakana-desktop-assistant\src\core\assistant.py | lines: 501-600 | window: 100
                await self.memory_manager.detect_pattern(
                    'successful_response',
                    {'response_type': response['type'], 'context': self.current_context}
                )
        
        # Evolutionary learning - improve response generation
        if len(self.conversation_history) % 10 == 0:  # Every 10 interactions
            await self._run_evolutionary_improvement()
    
    async def _run_evolutionary_improvement(self):
        """Run evolutionary algorithm to improve assistant behavior"""
        
        # Define fitness function based on user satisfaction
        async def fitness_function(genome: Dict[str, Any]) -> float:
            # Evaluate based on response time, success rate, etc.
            fitness = 0.0
            
            # Factor in success rate
            if self.metrics['requests_handled'] > 0:
                success_rate = self.metrics['successful_completions'] / self.metrics['requests_handled']
                fitness += success_rate * 50
            
            # Factor in response time (lower is better)
            if self.metrics['average_response_time'] > 0:
                fitness += 10 / self.metrics['average_response_time']
            
            # Factor in learning rate
            fitness += self.metrics['patterns_learned'] * 2
            
            return fitness
        
        # Initialize population with current configuration
        current_genome = {
            'temperature': self.config.temperature,
            'max_tokens': self.config.max_tokens,
            'learning_rate': self.config.learning_rate,
            'pattern_threshold': 0.7
        }
        
        self.evolutionary_learner.initialize_population(current_genome)
        
        # Run evolution for a few generations
        for _ in range(5):
            await self.evolutionary_learner.evaluate_population(fitness_function)
            self.evolutionary_learner.evolve_generation()
        
        # Apply best genome
        best_genome = self.evolutionary_learner.get_best_genome()
        self._apply_genome(best_genome)
        
        self.metrics['self_improvements'] += 1
        logger.info(f"Completed evolutionary improvement cycle {self.metrics['self_improvements']}")
    
    async def _propose_self_improvement(self):
        """Propose and test self-modifications"""
        
        # Example: Improve response generation
        modification = await self.self_modifier.propose_modification(
            target_function=self._generate_response,
            improvement_prompt="Improve response generation to be more concise and relevant",
            llm_interface=self.llm
        )
        
        if modification['success']:
            # Test the modification
            test_cases = [
                {
                    'inputs': {
                        'user_input': "What's the weather?",
                        'context': {}
                    },
                    'expected': {'type': 'text'}
                }
            ]
            
            test_result = await self.self_modifier.test_modification(
                modification,
                test_cases
            )
            
            if test_result['success']:
                self.self_modifier.apply_modification(modification)
                logger.info("Applied self-improvement modification")
    
    def _build_context(self, user_input: str, memories: List[Memory]) -> Dict[str, Any]:
        """Build context from memories and current state"""
        
        context = {
            'user_input': user_input,
            'timestamp': datetime.now().isoformat(),
            'conversation_history': self.conversation_history[-5:],  # Last 5 exchanges
            'relevant_memories': [
                {
                    'content': m.content,
                    'context': m.context,
                    'importance': m.importance_score
                }
                for m in memories
            ],
            'user_patterns': [],

# file: ..\sakana-desktop-assistant\src\core\assistant.py | lines: 601-700 | window: 100
            'current_context': self.current_context
        }
        
        return context
    
    def _build_prompt(self, user_input: str, context: Dict[str, Any]) -> str:
        """Build prompt for LLM with context"""
        
        # For Llama models, use proper chat format
        if hasattr(self.llm, 'llm') and hasattr(self.llm.llm, 'metadata'):
            # Llama chat format
            system_msg = """You are Sakana, an intelligent desktop assistant. Keep responses short and helpful.
Available commands: ls, cat, mkdir, find, sysinfo, cpu, memory, todo, search.
Answer in 1-3 sentences unless more detail is requested."""
            
            # Build conversation in chat format
            messages = [
                {"role": "system", "content": system_msg}
            ]
            
            # Add recent conversation history if available
            if context.get('conversation_history'):
                for exchange in context['conversation_history'][-2:]:
                    messages.append({"role": "user", "content": exchange['user']})
                    messages.append({"role": "assistant", "content": exchange['assistant']['content']})
            
            # Add current user input
            messages.append({"role": "user", "content": user_input})
            
            # Format for Llama 3.2
            prompt = "<|begin_of_text|>"
            for msg in messages:
                if msg["role"] == "system":
                    prompt += f"<|start_header_id|>system<|end_header_id|>\n\n{msg['content']}<|eot_id|>"
                elif msg["role"] == "user":
                    prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{msg['content']}<|eot_id|>"
                elif msg["role"] == "assistant":
                    prompt += f"<|start_header_id|>assistant<|end_header_id|>\n\n{msg['content']}<|eot_id|>"
            
            prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"
            
            return prompt
        
        # Fallback for other models
        prompt_parts = [
            "You are Sakana, an intelligent desktop assistant with self-learning capabilities.",
            f"Current time: {context['timestamp']}",
            ""
        ]
        
        if context['relevant_memories']:
            prompt_parts.append("Relevant information from memory:")
            for mem in context['relevant_memories'][:3]:
                prompt_parts.append(f"- {mem['content']}")
            prompt_parts.append("")
        
        if context['conversation_history']:
            prompt_parts.append("Recent conversation:")
            for exchange in context['conversation_history'][-2:]:
                prompt_parts.append(f"User: {exchange['user']}")
                prompt_parts.append(f"Assistant: {exchange['assistant']['content'][:100]}...")
            prompt_parts.append("")
        
        prompt_parts.append(f"User request: {user_input}")
        prompt_parts.append("")
        prompt_parts.append("Provide a helpful, concise response. If code is needed, include it in ```python blocks.")
        
        return "\n".join(prompt_parts)
    
    def _apply_genome(self, genome: Dict[str, Any]):
        """Apply evolved parameters"""
        
        if 'temperature' in genome:
            self.config.temperature = genome['temperature']
        if 'max_tokens' in genome:
            self.config.max_tokens = genome['max_tokens']
        if 'learning_rate' in genome:
            self.config.learning_rate = genome['learning_rate']
    
    def _update_avg_response_time(self, new_time: float):
        """Update average response time"""
        
        current_avg = self.metrics['average_response_time']
        count = self.metrics['successful_completions']
        
        self.metrics['average_response_time'] = (
            (current_avg * (count - 1) + new_time) / count
        )
    
    async def _load_user_profile(self):
        """Load user preferences and patterns from memory"""
        
        patterns = await self.memory_manager.get_patterns(min_confidence=0.7)
        
        for pattern in patterns:
            if pattern['pattern_type'] == 'time_preference':
                # Adjust behavior based on time preferences
                pass
            elif pattern['pattern_type'] == 'topic_interest':
                # Prioritize certain topics

# file: ..\sakana-desktop-assistant\src\core\assistant.py | lines: 701-800 | window: 100
                pass
        
        self.metrics['patterns_learned'] = len(patterns)
    
    # Utility methods
    def _classify_input(self, text: str) -> str:
        """Classify input type"""
        if any(word in text.lower() for word in ['?', 'what', 'how', 'why']):
            return 'question'
        elif any(word in text.lower() for word in ['create', 'make', 'build']):
            return 'command'
        else:
            return 'statement'
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extract keywords from text"""
        # Simple keyword extraction - could be improved with NLP
        words = text.lower().split()
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}
        return [w for w in words if w not in stopwords and len(w) > 3]
    
    def _extract_topics(self, text: str) -> List[str]:
        """Extract topics from text"""
        # Simplified topic extraction
        keywords = self._extract_keywords(text)
        return keywords[:3]  # Top 3 keywords as topics
    
    def _detect_task_type(self, user_input: str) -> Optional[str]:
        """Detect the type of task from user input"""
        
        input_lower = user_input.lower()
        
        # Document/file related
        if any(word in input_lower for word in ['read', 'document', 'file', 'summarize', 'summary', '.txt', '.pdf', '.doc']):
            return 'document_summarization'
        
        # File finding/searching
        elif any(word in input_lower for word in ['find', 'search', 'locate', 'where', 'look for']):
            return 'file_search'
        
        # Project related
        elif any(word in input_lower for word in ['project', 'repository', 'codebase', 'my code']):
            return 'project_exploration'
        
        # Command execution
        elif any(word in input_lower for word in ['run', 'execute', 'command', 'ls', 'cd', 'mkdir']):
            return 'command_execution'
        
        # Code generation
        elif any(word in input_lower for word in ['code', 'function', 'program', 'script', 'write code']):
            return 'code_generation'
        
        # Information retrieval
        elif any(word in input_lower for word in ['what', 'who', 'when', 'where', 'why', 'how', 'explain']):
            return 'information_retrieval'
        
        # Task management
        elif any(word in input_lower for word in ['todo', 'task', 'remind', 'schedule']):
            return 'task_management'
        
        return 'general'
    
    def _contains_code_request(self, response: Dict[str, Any]) -> bool:
        """Check if response contains code to execute"""
        return 'actions' in response and any(
            action.get('type') == 'execute_code' 
            for action in response['actions']
        )
    
    def _extract_code_from_response(self, response: Dict[str, Any]) -> Optional[str]:
        """Extract code from response"""
        content = response.get('content', '')
        
        # Look for code blocks
        if '```python' in content:
            start = content.find('```python') + 9
            end = content.find('```', start)
            return content[start:end].strip()
        
        return None
    
    def _merge_execution_result(self, response: Dict[str, Any], execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """Merge execution result into response"""
        
        if execution_result['success']:
            response['content'] += f"\n\nExecution result:\n{execution_result['output']}"
        else:
            response['content'] += f"\n\nExecution failed:\n{execution_result['error']}"
        
        response['execution_result'] = execution_result
        
        return response
    
    def _parse_response(self, response_text: str) -> Dict[str, Any]:
        """Parse LLM response for structured data"""
        
        # Try to extract JSON if present
        if '{' in response_text and '}' in response_text:
            try:
                start = response_text.find('{')

# file: ..\sakana-desktop-assistant\src\core\assistant.py | lines: 801-900 | window: 100
                end = response_text.rfind('}') + 1
                json_str = response_text[start:end]
                return json.loads(json_str)
            except:
                pass
        
        # Default response structure
        return {
            'content': response_text,
            'type': 'text'
        }
    
    async def shutdown(self):
        """Shutdown the assistant gracefully"""
        
        logger.info("Shutting down Sakana Assistant...")
        
        # Save current state
        if self.memory_manager:
            await self.memory_manager.close()
        
        if self.sandbox_executor:
            await self.sandbox_executor.cleanup()
        
        # Save metrics
        metrics_file = self.config.data_dir / "metrics.json"
        with open(metrics_file, 'w') as f:
            json.dump(self.metrics, f, indent=2)
        
        logger.info("Assistant shutdown complete")

    async def plan_and_execute(self, goal: str) -> Dict[str, Any]:
        """Plan and execute a small sequence of plugin commands from a natural-language goal.
        This uses lightweight heuristics and the existing PluginManager only (no external frameworks).
        Returns a dict with execution transcript.
        """
        if not self.plugin_manager:
            raise RuntimeError("Plugin system not initialized")

        available_cmds = {c['command'] for c in self.plugin_manager.get_available_commands()}

        steps: List[Dict[str, str]] = []
        text = goal.strip()

        # Heuristic: extract Windows paths and intents
        path_pattern = r"[A-Za-z]:\\[^\s\"]+"
        paths = re.findall(path_pattern, text)

        # Also extract relative paths like 'src\\delegation' or 'src/utils'
        rel_path_pattern = r"(?:\.?\.?|~|[A-Za-z0-9_.-]+)(?:[\\/][A-Za-z0-9_.-]+)+"
        rel_paths = re.findall(rel_path_pattern, text)
        if rel_paths:
            # Merge, preserving order: absolute first, then relatives
            paths = paths + rel_paths

        lower = text.lower()
        # mkdir intent: broaden phrasing detection
        if ("make a dir" in lower or "make directory" in lower or "mkdir" in lower or "create folder" in lower or "create a folder" in lower) and "mkdir" in available_cmds:
            if paths:
                steps.append({"command": "mkdir", "args": paths[0]})
        # list dir intent
        if ("list" in lower or "ls" in lower or "show files" in lower) and "ls" in available_cmds:
            # If we have any path (abs or rel), use it; otherwise special-case common tokens like 'src'
            if paths:
                target = paths[0]
            elif re.search(r"\bsrc\b", text, flags=re.IGNORECASE):
                target = "src"
            else:
                target = "."
            steps.append({"command": "ls", "args": target})
        # read file intent
        if ("read" in lower or "open" in lower or "cat" in lower) and "cat" in available_cmds and paths:
            steps.append({"command": "cat", "args": paths[-1]})

        # If no steps inferred but a single path present, and mkdir exists, default to mkdir
        if not steps and paths and "mkdir" in available_cmds:
            steps.append({"command": "mkdir", "args": paths[0]})

        if not steps:
            # Fallback: if user mentions 'docs' or 'dev', try to create a delegation folder under src
            project_root = self.config.base_dir
            delegation_dir = str(project_root / "src" / "delegation")
            if "mkdir" in available_cmds:
                steps.append({"command": "mkdir", "args": delegation_dir})

        transcript: List[Dict[str, Any]] = []
        context = self._build_context(goal, [])

        for step in steps:
            cmd = step["command"].strip()
            args = step.get("args", "").strip()
            user_input = f"{cmd} {args}".strip()
            try:
                result = await self.plugin_manager.handle_command(user_input, context)
                # Normalize result to a dict for consistent downstream handling
                if isinstance(result, str):
                    result = {"type": "info", "content": result}
                elif not isinstance(result, dict):
                    try:
                        import json as _json

# file: ..\sakana-desktop-assistant\src\core\assistant.py | lines: 901-920 | window: 100
                        result = {"type": "info", "content": _json.dumps(result, default=str)[:1000]}
                    except Exception:
                        result = {"type": "info", "content": str(result)[:1000]}
                if not result:
                    result = {"type": "error", "content": "No plugin handled the command."}
            except Exception as e:
                result = {"type": "error", "content": f"{type(e).__name__}: {e}"}
            transcript.append({"input": user_input, "result": result})

        summary_lines = [f"Executed {len(transcript)} step(s):"]
        for i, entry in enumerate(transcript, 1):
            summary_lines.append(f"{i}. {entry['input']} -> {entry['result'].get('type','info')}")
        summary = "\n".join(summary_lines)

        return {
            "success": True,
            "type": "info",
            "content": summary,
            "transcript": transcript
        }
