# EmbeddingGemma Desktop Setup üîç

Ein vollst√§ndiges Desktop-Interface f√ºr Google's EmbeddingGemma-300M Modell mit GUI, CLI und RAG-System.

## Abstract

EmbeddingGemma kombiniert lokale Embeddings mit einem Physarum-inspirierten Multi‚ÄëAgenten‚ÄëRetriever (MCMP) und optionaler Enterprise‚ÄëRAG‚ÄëSuche:
- **Embeddings + UI**: Klassische semantische Suche (Streamlit `app.py`) und MCMP‚ÄëRAG Oberfl√§che (`mcmp_streamlit.py`, `streamlit_fungus.py`).
- **MCMP‚ÄëRAG**: Viele Agenten bewegen sich im Embedding‚ÄëRaum, hinterlassen Pheromonspuren, d√§mpfen Trails und aktualisieren fortlaufend Dokument‚ÄëRelevanzen; am Ende werden Top‚ÄëK Chunks mit optionaler Diversit√§t zur√ºckgegeben.
- **Code‚ÄëSpace Frontend**: `streamlit_fungus.py` durchsucht Python‚ÄëRepos √ºber mehrstufige Chunks (Header: `# file: ‚Ä¶ | lines: a-b | window: w`), unterst√ºtzt Multi‚ÄëQuery, Auto‚ÄëGenerierung (LLM, grounded auf eingebetteten Dateien) und **Dedup** der Queries.
- **Agent‚ÄëChat & Tools**: Chat‚ÄëAgent mit Tool‚ÄëCalls (z. B. Code‚ÄëSuche, Root‚ÄëDir setzen), Hintergrund‚ÄëReports mit Live‚ÄëProgress und optionaler Snapshot‚ÄëGIF‚ÄëAufzeichnung.
- **Enterprise‚ÄëRAG**: Qdrant + LlamaIndex f√ºr persistente Indizes (Rag‚ÄëModus im Fungus‚ÄëUI), Hybrid‚ÄëScoring und Antwort‚ÄëGenerierung.
- **API & Coding Events**: `src/embeddinggemma/fungus_api.py` inkl. Endpoint zum Bauen von Code‚ÄëEdit‚ÄëEvents aus Chunk‚ÄëHeadern; optionales Publizieren in eine Queue.

## üöÄ Schnellstart

### 1. Setup ausf√ºhren
```bash
# Doppelklick auf setup.bat oder im Terminal:
setup.bat
```

### 2. Hugging Face Setup
- Bei [Hugging Face](https://huggingface.co) registrieren
- [EmbeddingGemma Lizenz](https://huggingface.co/google/embeddinggemma-300m) akzeptieren
- Falls n√∂tig: HF Token erstellen und setzen

### 3. Anwendung starten

#### üåê Web-Interface (Empfohlen)
```bash
streamlit run app.py
```
Dann Browser √∂ffnen: http://localhost:8501

#### üîç Kommandozeile
```bash
# Einfache Suche
python cli.py search "Wie funktioniert KI?" --docs "Machine Learning Tutorial" "Python Guide" "AI Basics"

# Similarity Matrix
python cli.py similarity "Text 1" "Text 2" "Text 3"

# Aus Datei suchen
python cli.py file-search "Deine Frage" documents.txt
```

#### üìö RAG System
```bash
# Demo starten
python rag.py demo

# Interaktive Suche
python rag.py interactive

# Dokumente hinzuf√ºgen und speichern
python rag.py add --file documents.txt --save meine_wissensbasis
```

## üîß Features

### Web-Interface (app.py)
- **Text Suche**: Query vs. Dokumente Similarity
- **Batch Analyse**: CSV-Dateien verarbeiten
- **Similarity Matrix**: Interaktive Heatmaps
- **Export Funktionen**: Cache-Management
- **Responsives Design**: Plotly Visualisierungen

### CLI Tool (cli.py)
- **Flexible Suche**: Ein-Zeilen-Kommandos
- **File-Support**: Textdateien durchsuchbar
- **JSON Export**: Strukturierte Ausgabe
- **Batch Processing**: Mehrere Texte gleichzeitig

### RAG System (rag.py)
- **Wissensbasis**: Dokumente persistent speichern
- **FAISS Integration**: Schnelle Vektorsuche
- **Interactive Mode**: Terminal-Chat
- **Metadata Support**: Erweiterte Dokumentinfos

## üìÅ Struktur

```
EmbeddingGemma/
‚îú‚îÄ‚îÄ app.py              # Streamlit Web-Interface
‚îú‚îÄ‚îÄ cli.py              # Kommandozeilen-Tool
‚îú‚îÄ‚îÄ rag.py              # RAG System
‚îú‚îÄ‚îÄ requirements.txt    # Python Dependencies
‚îú‚îÄ‚îÄ setup.bat          # Windows Setup-Script
‚îú‚îÄ‚îÄ embedding_cache/   # Model Cache
‚îú‚îÄ‚îÄ rag_cache/         # RAG Wissensbasis Cache
‚îî‚îÄ‚îÄ README.md          # Diese Datei
```

## üéØ Anwendungsf√§lle

### üìö Dokumentensuche
Durchsuche gro√üe Textsammlungen semantisch:
```bash
python rag.py add --file meine_dokumente.txt --save firmen_kb
python rag.py search "Wie implementiere ich Feature X?" --load firmen_kb
```

### üîç Content Discovery
Finde √§hnliche Inhalte in deiner Sammlung:
```bash
python cli.py similarity "Artikel 1" "Artikel 2" "Artikel 3" --output similarity.json
```

### ü§ñ AI-Pipeline Integration
Nutze als Embedding-Service f√ºr gr√∂√üere Systeme:
```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer("google/embeddinggemma-300m")
embeddings = model.encode(["Dein Text"])
```

## ‚öôÔ∏è Konfiguration

### Embedding-Dimensionen
- **768**: Beste Qualit√§t (Standard)
- **512**: Guter Kompromiss
- **256**: Schnell, weniger Speicher
- **128**: Sehr schnell, Mobile-optimiert

### Performance-Tipps
- **GPU**: Automatische CUDA-Nutzung falls verf√ºgbar
- **Batch-Size**: Bei gro√üen Datenmengen anpassen
- **Cache**: Wird automatisch verwendet
- **Dimensionen**: F√ºr Speed/Storage-Tradeoff reduzieren

## üêõ Troubleshooting

### Model Download Fehler
```bash
# HF Token setzen (falls n√∂tig)
huggingface-cli login
```

### CUDA Fehler
```bash
# CPU-only Installation
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

### Import Fehler
```bash
# Dependencies neu installieren
pip install -r requirements.txt --force-reinstall
```

## üîó Links

- [EmbeddingGemma Model](https://huggingface.co/google/embeddinggemma-300m)
- [Google AI Blog](https://developers.googleblog.com/en/introducing-embeddinggemma/)
- [Sentence Transformers](https://www.sbert.net/)
- [FAISS Documentation](https://faiss.ai/)

## üìä Model Info

- **Parameter**: 308M (100M Model + 200M Embedding)
- **Sprachen**: 100+ Sprachen
- **Context**: 2048 Token
- **Performance**: MTEB #1 unter 500M Parametern
- **Lizenz**: Gemma (kommerziell nutzbar)

## üèÉ‚Äç‚ôÇÔ∏è Quick Examples

### Web-Interface verwenden
1. `streamlit run app.py`
2. Text in "Suchanfrage" eingeben
3. Dokumente in "Dokumente" Bereich (eine Zeile = ein Dokument)
4. "Suche starten" klicken

### CLI f√ºr schnelle Tests
```bash
# Schnelle Similarity-Pr√ºfung
python cli.py search "Python Tutorial" --docs "Learn Python Programming" "JavaScript Guide" "Java Basics"

# Output als JSON
python cli.py search "AI Tutorial" --docs "ML Guide" "DL Basics" --output results.json
```

### RAG f√ºr Projekte
```bash
# Wissensbasis aufbauen
echo "Python ist eine Programmiersprache" > docs.txt
echo "Machine Learning nutzt Algorithmen" >> docs.txt
python rag.py add --file docs.txt --save projekt_kb

# Suchen
python rag.py search "Was ist Python?" --load projekt_kb
```

Viel Spa√ü beim Experimentieren mit EmbeddingGemma! üöÄ


## üß† MCMP-RAG: Schleimpilz-inspirierte Suche

**Neu!** Monte Carlo Physarum Machine (MCPM) f√ºr revolution√§re Dokumentenexploration, inspiriert vom Foraging-Verhalten des Schleimpilz *Physarum polycephalum*.

### üåü Was macht MCMP-RAG besonders?

**Statt statischer Vektorsuche:**
- **Adaptive Agenten** erkunden den Dokumentenraum dynamisch
- **Pheromonspur-basierte Pfadfindung** zwischen verwandten Dokumenten
- **Emergente Netzwerk-Strukturen** decken versteckte Verbindungen auf
- **Multi-hop Reasoning** √ºber mehrere Dokumente hinweg

### üöÄ MCMP-RAG verwenden

#### üåê Web-Interface
```bash
# MCMP-RAG mit Streamlit starten
streamlit run mcmp_streamlit.py

# Oder √ºber Batch-Script
start_mcmp.bat
```

#### üñ•Ô∏è Kommandozeile
```bash
# Schnelle Suche
python mcmp_cli.py search "Wie funktioniert KI?" --docs "AI Tutorial" "ML Guide" "DL Basics"

# Interaktiver Modus
python mcmp_cli.py interactive

# Benchmark verschiedener Konfigurationen
python mcmp_cli.py benchmark --file documents.txt --queries "Query 1" "Query 2"

# Demo ausf√ºhren
python mcmp_cli.py demo
```

#### üß™ Python API
```python
from mcmp_rag import MCPMRetriever

# MCMP System initialisieren
mcmp = MCPMRetriever(num_agents=500, max_iterations=100)

# Dokumente hinzuf√ºgen
documents = ["Doc 1", "Doc 2", "Doc 3"]
mcmp.add_documents(documents)

# Suche mit Schleimpilz-Algorithmus
results = mcmp.search("Meine Frage", top_k=5)

# Ergebnisse analysieren
for result in results['results']:
    print(f"[{result['relevance_score']:.3f}] {result['content']}")

# Netzwerk visualisieren
mcmp.visualize_search_process("search_analysis.png")
```

### üîß MCMP Parameter

| Parameter | Standard | Beschreibung |
|-----------|----------|-------------|
| `num_agents` | 300 | Anzahl MCPM-Agenten |
| `max_iterations` | 80 | Maximale Suchiterationen |
| `pheromone_decay` | 0.95 | Pheromonspur-Abklingrate |
| `exploration_bonus` | 0.1 | Exploration vs. Exploitation |

### üìä MCMP vs. Standard RAG

| Feature | Standard RAG | MCMP-RAG |
|---------|-------------|----------|
| **Suchstrategie** | Statische Vektorsuche | Adaptive Agenten-Exploration |
| **Dokumentverbindungen** | Keine | Emergente Netzwerke |
| **Pfadfindung** | Single-hop | Multi-hop Reasoning |
| **Lernf√§higkeit** | Statisch | Dynamische Anpassung |
| **√úberraschungsfaktor** | Gering | Hoch (Serendipity) |

### üéØ Anwendungsf√§lle

#### üìö **Akademische Forschung**
- Literaturrecherche mit versteckten Verbindungen
- Cross-Domain Wissenstransfer
- Interdisziplin√§re Konzeptentdeckung

#### üè¢ **Unternehmens-Wissensbasen**
- Komplexe Policy-Suche
- Innovative L√∂sungsans√§tze
- Team-√ºbergreifende Expertise-Findung

#### üîç **Content Discovery**
- Unerwartete thematische Verbindungen
- Content-Gap-Analyse
- Trend-Vorhersage durch Netzwerk-Analyse

### üìà Performance-Tipps

```python
# F√ºr gro√üe Dokumentensammlungen (>1000 Docs)
mcmp = MCPMRetriever(
    num_agents=1000,       # Mehr Agenten f√ºr bessere Abdeckung
    max_iterations=150,    # L√§ngere Exploration
    exploration_bonus=0.2  # Mehr Zuf√§lligkeit
)

# F√ºr schnelle Prototyping
mcmp = MCPMRetriever(
    num_agents=100,        # Weniger Agenten
    max_iterations=50,     # K√ºrzere Suche
    exploration_bonus=0.05 # Fokussierte Suche
)
```

### üß¨ Algorithmus-Details

MCPM basiert auf dem Foraging-Verhalten von *Physarum polycephalum*:

1. **Agent Spawning**: Agenten starten um Query-Embedding
2. **Attraction Forces**: Bewegung zu relevanten Dokumenten
3. **Pheromone Deposition**: Erfolgreiche Pfade werden markiert
4. **Trail Following**: Andere Agenten folgen starken Spuren
5. **Network Emergence**: Stabile Verbindungsstrukturen entstehen
6. **Relevance Extraction**: Finale Dokumenten-Rankings

### üìñ Inspiration & Referenzen

- **Polyphorm Paper**: [Polyphorm: Structural Analysis of Cosmological Datasets](https://arxiv.org/abs/2009.02441)
- **Original Repository**: [CreativeCodingLab/Polyphorm](https://github.com/CreativeCodingLab/Polyphorm)
- **Physarum-Algorithmen**: Jones, Jeff (2010). "Characteristics of pattern formation and evolution in approximations of Physarum transport networks"

---

## ‚úÖ TODO: Coding Events & Queue Integration

Diese Sektion markiert anstehende Arbeiten rund um Code-Edit-Events, damit der Haupt‚ÄëAgent (Producer) eine Redis‚ÄëQueue speisen kann und ein Coder‚ÄëAgent (Consumer) gezielte Edits ausf√ºhrt.

- [ ] API vervollst√§ndigen: Code‚ÄëEdit‚ÄëEvent bauen und optional ver√∂ffentlichen
  - Endpoint (bereits vorhanden): `POST /api/edit/build_event` in `src/embeddinggemma/fungus_api.py`
  - Zweck: Aus einem RAG‚ÄëChunk‚ÄëHeader `# file: <pfad> | lines: a-b | window: w` exakte Dateipfade und Zeilengrenzen extrahieren, optional via AST auf eine Methode einschr√§nken, und ein neutrales Edit‚ÄëEvent erzeugen.
- [ ] Redis‚ÄëIntegration finalisieren
  - Env: `REDIS_URL`, `CODER_EVENTS_KEY` (Standard: `coder:events`)
  - Zweck: Bei `publish=true` das Event als JSON in die Queue pushen, damit der Coder‚ÄëAgent es konsumieren kann.
- [ ] Event‚ÄëSchema konsolidieren (Platzhalter ‚Äì Werte sp√§ter bef√ºllen)
  ```json
  {
    "id": "<uuid>",
    "type": "code_edit_request",
    "file_path": "<pfad.py>",
    "start_line": <int>,
    "end_line": <int>,
    "instructions": "<was zu √§ndern ist>",
    "before": "<optional: aktueller Code>",
    "bounds_source": "<chunk_header|explicit>",
    "chunk_header": "# file: <pfad> | lines: <a>-<b> | window: <w>",
    "prefer_method": "<optional: funktionsname>",
    "meta": { "query": "<ursprungsfrage>", "score": <float>, "source": "<rag|mcmp|manual>" },
    "routing": { "return_queue": "coder:results", "notify_webhook": "<optional_url>" },
    "control": { "priority": "<low|normal|high>", "timeout_s": <int>, "dry_run": <true|false> },
    "correlation_id": "<trace/run id>"
  }
  ```
- [ ] Client‚ÄëSnippet (Beispiel):
  ```bash
  curl -X POST http://localhost:8055/api/edit/build_event \
    -H "Content-Type: application/json" \
    -d '{
      "chunk_text": "# file: src\\embeddinggemma\\rag.py | lines: 1-277 | window: 800\n...",
      "prefer_method": "search",
      "instructions": "Refaktor: Benenne Variable x in query_text um.",
      "publish": true,
      "extra": {"issue_id": 123}
    }'
  ```

Hinweis: Separator‚ÄëLinien (`-----`) k√∂nnen in Datendateien f√ºr Lesbarkeit genutzt werden. F√ºr das eigentliche Embedding oder die Edit‚ÄëGrenzen werden die Header‚ÄëMetadaten bevorzugt, weil sie exakte Datei‚Äë und Zeilenbereiche liefern.

