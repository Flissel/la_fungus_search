# EmbeddingGemma Desktop Setup ğŸ”

Ein vollstÃ¤ndiges Desktop-Interface fÃ¼r Google's EmbeddingGemma-300M Modell mit GUI, MCMP-RAG, RagV1 und Agent-Chat.

## Abstract

EmbeddingGemma kombiniert lokale Embeddings mit einem Physarum-inspirierten Multiâ€‘Agentenâ€‘Retriever (MCMP) und optionaler Enterpriseâ€‘RAGâ€‘Suche:
- **Embeddings + UI**: Klassische semantische Suche und MCMPâ€‘RAG OberflÃ¤che (`streamlit_fungus_backup.py`).
- **MCMPâ€‘RAG**: Viele Agenten bewegen sich im Embeddingâ€‘Raum, hinterlassen Pheromonspuren, dÃ¤mpfen Trails und aktualisieren fortlaufend Dokumentâ€‘Relevanzen; am Ende werden Topâ€‘K Chunks mit optionaler DiversitÃ¤t zurÃ¼ckgegeben.
- **Codeâ€‘Space Frontend**: `streamlit_fungus_backup.py` durchsucht Pythonâ€‘Repos Ã¼ber mehrstufige Chunks (Header: `# file: â€¦ | lines: a-b | window: w`), unterstÃ¼tzt Multiâ€‘Query (LLMâ€‘generiert, grounded) und **Dedup**.
- **Agentâ€‘Chat & Tools**: Chatâ€‘Agent mit Toolâ€‘Calls (z. B. Codeâ€‘Suche, Rootâ€‘Dir setzen), Hintergrundâ€‘Reports mit Liveâ€‘Progress und optionaler Snapshotâ€‘GIFâ€‘Aufzeichnung.
- **Enterpriseâ€‘RAG**: Qdrant + LlamaIndex fÃ¼r persistente Indizes (Ragâ€‘Modus im Fungusâ€‘UI), Hybridâ€‘Scoring und Antwortâ€‘Generierung.
- **Realtime API Server**: `src/embeddinggemma/realtime/server.py` FastAPI WebSocket server fÃ¼r Live-MCMP-Simulation mit React Frontend.

Siehe auch:
- Architektur & C4: docs/ARCHITECTURE.md
- Scripts overview: docs/SCRIPTS.md
- Simulation Details: docs/mcmp_simulation.md
- API Reference: docs/API_REFERENCE.md
- Config Reference: docs/CONFIG_REFERENCE.md

## ğŸš€ Schnellstart

### 1. Setup ausfÃ¼hren
```bash
# Install dependencies
pip install -r requirements.txt

# Optional: Editable install
pip install -e .
```

### 2. Hugging Face Setup
- Bei [Hugging Face](https://huggingface.co) registrieren
- [EmbeddingGemma Lizenz](https://huggingface.co/google/embeddinggemma-300m) akzeptieren
- Falls nÃ¶tig: HF Token erstellen und setzen

### 3. Anwendung starten

#### ğŸŒ Streamlit Interface (Empfohlen)
```bash
# Direct run
streamlit run streamlit_fungus_backup.py

# Or using helper scripts
.\run-streamlit.ps1    # PowerShell
run-streamlit.cmd      # Windows CMD
```
Dann Browser Ã¶ffnen: http://localhost:8501

#### ğŸš€ Realtime WebSocket Server + React Frontend
```bash
# 1. Start FastAPI server
uvicorn src.embeddinggemma.realtime.server:app --reload --port 8011

# 2. Start React frontend (separate terminal)
cd frontend
npm install  # first time only
npm run dev
```
- Server: http://localhost:8011  
- React Frontend: http://localhost:5173

#### ğŸ› ï¸ RAG CLI (verfÃ¼gbar in `experimerntal/old/`)
```bash
# Index aufbauen 
python experimerntal/old/rag_v1.py build --directory src

# Query gegen Index
python experimerntal/old/rag_v1.py query "RAG implementation details" --top-k 5
```

## ğŸ”§ Features

### Streamlit Interface (streamlit_fungus_backup.py)
- Text/Code Suche Ã¼ber mehrstufige Chunks
- Multi-Query (LLM generiert, grounded auf eingebetteten Dateien)
- Dedup der Queries (Jaccard)
- Live-Logging, GIF-Snapshots, Agent-Chat (Tool-Calls)
- "Rag"-Sektion fÃ¼r Enterprise RAG (Qdrant + LlamaIndex)

### Realtime WebSocket Server (realtime/server.py)
- FastAPI mit WebSocket Live-Updates
- Pause/Resume/Reset Simulation
- Agents dynamisch hinzufÃ¼gen/anpassen 
- Live Top-K Ergebnisse und Visualisierung
- Background Reports mit LLM Integration
- REST API fÃ¼r alle Konfigurationen

### RAG Module (rag/ directory)
- ASTâ€‘Chunking fÃ¼r Code
- Hybrid Retrieval (semantic + keyword)
- Qdrant VectorStore Integration
- Generierung via HF LLM oder Ollama

## ğŸ“ Struktur
```
EmbeddingGemma/
â”œâ”€â”€ streamlit_fungus_backup.py         # Streamlit Frontend (MCMP + RAG + Agent Chat)
â”œâ”€â”€ src/embeddinggemma/
â”‚   â”œâ”€â”€ realtime/server.py             # FastAPI WebSocket Server
â”‚   â”œâ”€â”€ mcmp_rag.py                   # Core MCMP Retriever
â”‚   â”œâ”€â”€ mcmp/                         # Simulation, embeddings, PCA
â”‚   â”œâ”€â”€ rag/                          # RAG components (chunking, vectorstore)
â”‚   â”œâ”€â”€ ui/                           # UI components for Streamlit
â”‚   â””â”€â”€ agents/agent_fungus_rag.py    # Agent with tool calls
â”œâ”€â”€ frontend/                         # React frontend for realtime server
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ SCRIPTS.md
â”‚   â”œâ”€â”€ mcmp_simulation.md
â”‚   â”œâ”€â”€ CONFIG_REFERENCE.md
â”‚   â””â”€â”€ API_REFERENCE.md
â””â”€â”€ README.md
```

## âš™ï¸ Konfiguration

### Umgebungsvariablen
- `OLLAMA_MODEL`: LLM Model (default: qwen2.5-coder:7b)
- `OLLAMA_HOST`: Ollama Server URL (default: http://127.0.0.1:11434)
- `HF_TOKEN`: Hugging Face Token (falls benÃ¶tigt)

### Hardware
- GPU fÃ¼r Embeddings (HF) empfohlen; MCMP Simulation lÃ¤uft auf CPU
- Minimum 8GB RAM fÃ¼r grÃ¶ÃŸere Korpora

### Ports
- Streamlit: 8501 (default)
- Realtime Server: 8011 (empfohlen)
- Frontend Dev: 5173 (Vite)
- Ollama: 11434 (default)

## ğŸ§  MCMP-RAG: Schleimpilz-inspirierte Suche

MCMP erkundet den Dokumentenraum adaptiv, verstÃ¤rkt Pfade durch Pheromone und bildet ein Netzwerk relevanter Verbindungen (multiâ€‘hop). Das ermÃ¶glicht AbhÃ¤ngigkeitsâ€‘Tracing Ã¼ber Dateien hinweg â€“ ergÃ¤nzend zur klassischen NNâ€‘Suche.

### Parameter (Beispiele)
- num_agents: 300
- max_iterations: 80
- pheromone_decay: 0.95
- exploration_bonus: 0.1

## ğŸ”— Links
- [EmbeddingGemma Model](https://huggingface.co/google/embeddinggemma-300m)
- [Polyphorm (Inspiration)](https://github.com/CreativeCodingLab/Polyphorm)
- [LlamaIndex](https://www.llamaindex.ai/)
- [Qdrant](https://qdrant.tech/)

