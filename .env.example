# LA Fungus Search - Environment Configuration Template
# Copy this file to .env and fill in your actual values

# =============================================================================
# LLM PROVIDER SELECTION
# =============================================================================
# Choose which LLM provider to use for judge/report/answer operations
# Options: ollama (local), openai, google, grok
# LLM_PROVIDER=ollama

# =============================================================================
# REQUIRED: OpenAI API Configuration (if using OpenAI provider)
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-your_api_key_here
OPENAI_BASE_URL=https://api.openai.com
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_TEMPERATURE=0.0

# =============================================================================
# OPTIONAL: Ollama (Local LLM)
# =============================================================================
# Install Ollama from: https://ollama.ai
# Use Ollama for local LLM inference (alternative to OpenAI)

# OLLAMA_MODEL=qwen2.5-coder:7b
# OLLAMA_HOST=http://127.0.0.1:11434
# OLLAMA_SYSTEM=You are a precise code analysis assistant. Output strict JSON as requested.
# OLLAMA_NUM_GPU=1
# OLLAMA_NUM_THREAD=12
# OLLAMA_NUM_BATCH=128

# =============================================================================
# OPTIONAL: Google Generative AI
# =============================================================================
# Use Google's Gemini models

# GOOGLE_MODEL=gemini-1.5-pro
# GOOGLE_API_KEY=your_google_api_key_here
# GOOGLE_BASE_URL=https://generativelanguage.googleapis.com
# GOOGLE_TEMPERATURE=0.0

# =============================================================================
# OPTIONAL: Grok (xAI)
# =============================================================================
# Use xAI's Grok models

# GROK_MODEL=grok-2-latest
# GROK_API_KEY=your_grok_api_key_here
# GROK_BASE_URL=https://api.x.ai
# GROK_TEMPERATURE=0.0

# =============================================================================
# OPTIONAL: Embedding Model Configuration
# =============================================================================
# Specify which model to use for embeddings
# If not set and OPENAI_API_KEY is present, defaults to openai:text-embedding-3-large
# Otherwise defaults to google/embeddinggemma-300m

# EMBEDDING_MODEL=openai:text-embedding-3-large
# DEVICE_MODE=auto

# Available options for DEVICE_MODE:
# - auto: automatically detect best device
# - cpu: force CPU usage
# - cuda: use GPU acceleration (if available)

# =============================================================================
# OPTIONAL: Qdrant Vector Database
# =============================================================================
# Use Qdrant for persistent vector storage (recommended for large codebases)
# Start Qdrant with: docker-compose -f docker-compose.qdrant.yml up -d
# Leave commented to use in-memory FAISS (default)

# VECTOR_BACKEND=qdrant
# QDRANT_URL=http://localhost:6339
# QDRANT_API_KEY=your_qdrant_api_key_here
# QDRANT_COLLECTION=codebase

# =============================================================================
# OPTIONAL: Supermemory (LLM Judge Memory)
# =============================================================================
# Supermemory provides persistent memory for the LLM judge during exploration
# Get your API key from: https://supermemory.ai
# Leave commented to disable memory features

#SUPERMEMORY_API_KEY=

# Custom Supermemory Base URL (optional)
# Defaults to: https://api.supermemory.ai
# Only set this if using a self-hosted or custom Supermemory instance
#SUPERMEMORY_BASE_URL=https://api.supermemory.ai
# =============================================================================
# OPTIONAL: Server Configuration
# =============================================================================
# These have sensible defaults but can be overridden

# Backend server port
# EMBEDDINGGEMMA_BACKEND_PORT=8011

# Frontend development server port
# VITE_BACKEND_PORT=8011

# Logging
# LOG_LEVEL=INFO

# =============================================================================
# SECURITY NOTES
# =============================================================================
# - NEVER commit .env file to version control
# - Add .env to .gitignore (already done)
# - Rotate API keys regularly
# - Use environment-specific .env files for dev/staging/prod
# - Consider using secret management tools for production
#
# - IMPORTANT: Keep your API keys secure and never share them
# - Different providers have different pricing models - review costs before use
# - Ollama is free and runs locally but requires sufficient hardware resources

# =============================================================================
# QUICK START EXAMPLES
# =============================================================================
# Example 1: Using OpenAI (cloud-based, requires API key)
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-proj-your_actual_key_here
# OPENAI_MODEL=gpt-4o-mini
# EMBEDDING_MODEL=openai:text-embedding-3-large

# Example 2: Using Ollama (local, free)
# LLM_PROVIDER=ollama
# OLLAMA_MODEL=qwen2.5-coder:7b
# OLLAMA_HOST=http://127.0.0.1:11434
# EMBEDDING_MODEL=google/embeddinggemma-300m
# DEVICE_MODE=cuda

# Example 3: Using Qdrant for persistent storage
# VECTOR_BACKEND=qdrant
# QDRANT_URL=http://localhost:6339
# QDRANT_COLLECTION=my_codebase
# (Don't forget to start Qdrant: docker-compose -f docker-compose.qdrant.yml up -d)
